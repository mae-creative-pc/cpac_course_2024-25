{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yogdziex7Gf7"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gf_HUOab7Gf8"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QwQk8tRyAcv"
   },
   "source": [
    "# Pix2Pix\n",
    "\n",
    "Pix to Pix is a technique for image translation using conditional GANs. In this way we can colorize black and white photos, convert google maps to google earth, etc.\n",
    "\n",
    "We are going to convert building facades drawing to real buildings.\n",
    "\n",
    "In example, we will use the [CMP Facade Database](http://cmp.felk.cvut.cz/~tylecr1/facade/), helpfully provided by the [Center for Machine Perception](http://cmp.felk.cvut.cz/) at the [Czech Technical University in Prague](https://www.cvut.cz/).\n",
    "\n",
    "We will use a preprocessed [copy](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/) of this dataset, created by the authors of the [paper](https://arxiv.org/abs/1611.07004) above.\n",
    "\n",
    "Each epoch takes around 15 seconds on a single V100 GPU.\n",
    "\n",
    "Below is the output generated after training the model for 200 epochs.\n",
    "\n",
    "![sample output_1](https://www.tensorflow.org/images/gan/pix2pix_1.png)\n",
    "![sample output_2](https://www.tensorflow.org/images/gan/pix2pix_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frXYU-Gh9dNZ"
   },
   "source": [
    "# Load the dataset\n",
    " the dataset used in this tutorial and similar ones can be downloaded from [this page](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/):\n",
    "* cityscapes.tar.gz\t \n",
    "* edges2handbags.tar.gz\t\n",
    "* edges2shoes.tar.gz\t \n",
    "* facades.tar.gz\t\n",
    "* maps.tar.gz \n",
    "\n",
    "We apply random jittering and mirroring to the training dataset:\n",
    "  * random jittering: image is resized to 286 x 286 then randomly cropped to 256x256\n",
    " * random mirroring: image randomly flipped horizontally i.e. left to right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_-JsK4HiWMz"
   },
   "outputs": [],
   "source": [
    "dataset_name = \"facades\" #@param [\"cityscapes\", \"edges2handbags\", \"edges2shoes\", \"facades\", \"maps\", \"night2day\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nr9Yw9F-Gnh"
   },
   "outputs": [],
   "source": [
    "_URL = f'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/{dataset_name}.tar.gz'\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    fname=f\"{dataset_name}.tar.gz\",\n",
    "    origin=_URL,\n",
    "    extract=True)\n",
    "\n",
    "path_to_zip  = pathlib.Path(path_to_zip)\n",
    "\n",
    "\n",
    "PATH = path_to_zip.parent/dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WF3654DNN0Mb"
   },
   "source": [
    "Inspect one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24xHQx9mNdl2"
   },
   "outputs": [],
   "source": [
    "sample_image = tf.io.read_file(os.path.join(PATH,'train/1.jpg'))\n",
    "sample_image = tf.io.decode_jpeg(sample_image)\n",
    "print(sample_image.shape)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(sample_image,aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJToabWq-yqf"
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "BUFFER_SIZE = 400\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAWuUiaF-2ZP"
   },
   "outputs": [],
   "source": [
    "# Load and split image between real and input\n",
    "def load(image_file):\n",
    "  image = tf.io.read_file(image_file)\n",
    "  image = tf.image.decode_jpeg(image)\n",
    "\n",
    "  w = tf.shape(image)[1]\n",
    "\n",
    "  w = w // 2\n",
    "  real_image = image[:, :w, :]\n",
    "  input_image = image[:, w:, :]\n",
    "\n",
    "  input_image = tf.cast(input_image, tf.float32)\n",
    "  real_image = tf.cast(real_image, tf.float32)\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qv3YcOU_-_eI"
   },
   "outputs": [],
   "source": [
    "# Show images\n",
    "inp, re = load(os.path.join(PATH,'train/120.jpg'))\n",
    "# casting to int for matplotlib to show the image\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(121)\n",
    "plt.imshow(inp/255.0, aspect='auto')\n",
    "plt.title('Input',fontdict={'fontsize':20})\n",
    "plt.subplot(122)\n",
    "plt.imshow(re/255.0, aspect='auto')\n",
    "plt.title('Real',fontdict={'fontsize':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xt6pfTfvAX3v"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOWfrnQ-ACAG"
   },
   "outputs": [],
   "source": [
    "def resize(input_image, real_image, height, width):\n",
    "  input_image = tf.image.resize(input_image, [height, width],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "  real_image = tf.image.resize(real_image, [height, width],\n",
    "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Obl4V2ZNAKeF"
   },
   "outputs": [],
   "source": [
    "def random_crop(input_image, real_image):\n",
    "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "  cropped_image = tf.image.random_crop(\n",
    "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "\n",
    "  return cropped_image[0], cropped_image[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6T9MV0hqALsa"
   },
   "outputs": [],
   "source": [
    "# normalizing the images to [-1, 1]\n",
    "def normalize(input_image, real_image):\n",
    "  input_image = (input_image / 127.5) - 1\n",
    "  real_image = (real_image / 127.5) - 1\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIq-7GTCARr1"
   },
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def random_jitter(input_image, real_image):\n",
    "  # resizing to 286 x 286 x 3\n",
    "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
    "\n",
    "  # randomly cropping to 256 x 256 x 3\n",
    "  input_image, real_image = random_crop(input_image, real_image)\n",
    "\n",
    "  if tf.random.uniform(()) > 0.5:\n",
    "    # random mirroring\n",
    "    input_image = tf.image.flip_left_right(input_image)\n",
    "    real_image = tf.image.flip_left_right(real_image)\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2UiVd7KAn30"
   },
   "source": [
    "Random jittering does the following:\n",
    "* Resize an image to a bigger height and width\n",
    "* Randomly crop to target size\n",
    "* Randomly flip the image horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NM5L-4tQAj-_"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(4):\n",
    "  rj_inp, rj_re = random_jitter(inp, re)\n",
    "  plt.subplot(2, 2, i+1)\n",
    "  plt.imshow(rj_inp/255.0)\n",
    "  plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLQ9_UJ7A04m"
   },
   "outputs": [],
   "source": [
    "def load_image_train(image_file):\n",
    "  input_image, real_image = load(image_file)\n",
    "  input_image, real_image = random_jitter(input_image, real_image)\n",
    "  input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HjDpJhNA4rK"
   },
   "outputs": [],
   "source": [
    "def load_image_test(image_file):\n",
    "  input_image, real_image = load(image_file)\n",
    "  input_image, real_image = resize(input_image, real_image,\n",
    "                                   IMG_HEIGHT, IMG_WIDTH)\n",
    "  input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "  return input_image, real_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piHz515xA6uo"
   },
   "source": [
    "# Input Pipeline\n",
    "\n",
    "In this example, we will use the [`tf.data`](https://www.tensorflow.org/guide/data) API, while it is not necessary to understand everything for the sake of this tutorial, check the link if you want to know more about how it works.\n",
    "\n",
    "In a nutshell it is an API which allows you to handle data and feed them into tensorflow models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWP7kgS-A9A6"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.list_files(os.path.join(PATH,'train/*.jpg'))\n",
    "train_dataset = train_dataset.map(load_image_train,\n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yir10d56A_PY"
   },
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.list_files(os.path.join(PATH,'test/*.jpg'))\n",
    "test_dataset = test_dataset.map(load_image_test)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOOnd_xKzK-J"
   },
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4Mu1gZIBFN2"
   },
   "source": [
    "# Build the generator\n",
    "  * Architecture is a modified [U-Net](https://arxiv.org/abs/1505.04597) \n",
    "  \n",
    "  (You can find out more about the implementation [here](https://www.tensorflow.org/tutorials/images/segmentation))\n",
    "  * Each block in the encoder is (Conv -> [Batchnorm](https://arxiv.org/abs/1502.03167) -> [Leaky ReLU](https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf))\n",
    "  * There are [skip connections](https://en.wikipedia.org/wiki/Residual_neural_network) between the encoder and decoder (as in U-Net).\n",
    "\n",
    "\n",
    "  ![unet_architecture](https://machinelearningmastery.com/wp-content/uploads/2019/05/Architecture-of-the-U-Net-Generator-Model.png)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfmH1F-3BH93"
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3 # RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJ_CKHc0By-u"
   },
   "source": [
    "## Downsampling layer: FILL THE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvisNp4wBh3y"
   },
   "outputs": [],
   "source": [
    "#FILL THE CODE: complete the downsampling layer that we will use to shrink the image input\n",
    "\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  #FILL THE CODE: Add a 2D convolutional layer, with the specified options, use strides=2, padding=\"same\" and use_bias=False\n",
    "  result.add(\n",
    "      #...... TODO.......\n",
    "  )\n",
    "  if apply_batchnorm:\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Grl0t-nPBlYp"
   },
   "outputs": [],
   "source": [
    "down_model = downsample(3, 4)\n",
    "down_result = down_model(tf.expand_dims(inp, 0))\n",
    "print (down_result.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(inp, aspect='auto')\n",
    "plt.subplot(122)\n",
    "plt.imshow(down_result[0], aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGInXDbsB3Yu"
   },
   "source": [
    "## Upsampling Layer: FILL THE CODE\n",
    "\n",
    "To upsample the images we are going to use a [`Conv2DTranspose`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) layer, which performs a [transposed convolution](https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11#:~:text=Transposed%20convolutions%20are%20standard%20convolutions,in%20a%20standard%20convolution%20operation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZkXG5y8B5fV"
   },
   "outputs": [],
   "source": [
    "#FILL THE CODE: complete the upsampling layer that we will increase image size after downsampling it\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  #FILL THE CODE: Add a 2D transposed convolutional layer, with the specified options, use strides=2, padding='same' and use_bias=False \n",
    "\n",
    "  result.add(\n",
    "      # ...Conv2DTranspose layer\n",
    "      # ... TODO....\n",
    "  )\n",
    "\n",
    "  result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  if apply_dropout:\n",
    "      result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arKC_Bm3CDZ_"
   },
   "outputs": [],
   "source": [
    "up_model = upsample(3, 4)\n",
    "up_result = up_model(down_result)\n",
    "print (up_result.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(down_result[0], aspect='auto')\n",
    "plt.subplot(122)\n",
    "plt.imshow(up_result[0], aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cw3MrUqq522f"
   },
   "source": [
    "## Build the Generator by combining downsampling and upsampling layers and implementing the  skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYqkvD-vCJTH"
   },
   "outputs": [],
   "source": [
    "def Generator():\n",
    "  inputs = tf.keras.layers.Input(shape=[256,256,3])\n",
    "\n",
    "  # Downsampling layers\n",
    "  down_stack = [\n",
    "    downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
    "    downsample(128, 4), # (bs, 64, 64, 128)\n",
    "    downsample(256, 4), # (bs, 32, 32, 256)\n",
    "    downsample(512, 4), # (bs, 16, 16, 512)\n",
    "    downsample(512, 4), # (bs, 8, 8, 512)\n",
    "    downsample(512, 4), # (bs, 4, 4, 512)\n",
    "    downsample(512, 4), # (bs, 2, 2, 512)\n",
    "    downsample(512, 4), # (bs, 1, 1, 512)\n",
    "  ]\n",
    "\n",
    "  # Upsampling layers\n",
    "  up_stack = [\n",
    "    upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
    "    upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
    "    upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
    "    upsample(512, 4), # (bs, 16, 16, 1024)\n",
    "    upsample(256, 4), # (bs, 32, 32, 512)\n",
    "    upsample(128, 4), # (bs, 64, 64, 256)\n",
    "    upsample(64, 4), # (bs, 128, 128, 128)\n",
    "  ]\n",
    "\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  # Last layer\n",
    "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh') # (bs, 256, 256, 3)\n",
    "\n",
    "\n",
    "\n",
    "  # FILL THE CODE\n",
    "  # Considering x as the input, try to build the model such that:\n",
    "  # For the Encoder layers you simply apply the downsampling layers\n",
    "  # For the Decoder layers:\n",
    "  # the nth layer of the decoder takes as input the concatenation (tf.keras.layers.Concatenate) of the output of the n-1th decoder layer and n-num_decoder_layers +1 encoder layer\n",
    "  # e.g. the last layer of the decoder gets as input the output of the previous layer concatenated with the output of the first encoder convolutional layer (!!not the input layer)\n",
    "  \n",
    "  # Downsampling through the model\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = # FILL THE CODE\n",
    "    skips.#... TODO ...\n",
    "\n",
    "  skips = #... TODO ...\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = #... TODO ...\n",
    "    x = #... TODO ...\n",
    "\n",
    "  x = #... TODO ...\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8ewAk_8CNcq"
   },
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFkM2szGCdh8"
   },
   "outputs": [],
   "source": [
    "gen_output = generator(inp[tf.newaxis,...], training=False) # N.B. inp is input image\n",
    "plt.imshow(gen_output[0,...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iBl8eQrDh45"
   },
   "source": [
    "   ## Generator Loss: FILL THE CODE\n",
    "    * sigmoid cross entropy loss of generated images and and array of ones (classification)\n",
    "    * L1 loss -> Mean Absolute Error (MAE) between generated image and target image\n",
    "      * this allows generated image to be structurally similar to target image\n",
    "      * Formula to calculate total generator loss is:\n",
    "        * $ \\mathcal{L}_\\text{gan} +\\lambda \\mathcal{L}_\\text{L1} $ \n",
    "        \n",
    "        with $\\lambda = 100$ (decided by authors of the original paper)\n",
    "     \n",
    "\n",
    "```\n",
    "# Questo Ã¨ formattato come codice\n",
    "```\n",
    "\n",
    "\n",
    "The training procedure for the generator is shown below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uj3fMvhqEINA"
   },
   "outputs": [],
   "source": [
    "LAMBDA = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qARKmxRGzgr"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jvp4M_VERHQ"
   },
   "outputs": [],
   "source": [
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "  # FILL THE CODE:\n",
    "  # define the GAN generator loss i.e. does the discriminator classify generator \n",
    "  # outputs as real?\n",
    "  # HINT: use loss_object to compute the loss and tf.ones_like() to represent classification\n",
    "  # result\n",
    "\n",
    "  #gan_loss =  ...TODO...\n",
    "\n",
    "  # FILL THE CODE: mean absolute error\n",
    "  # compute mean absolute error between target image and generated image\n",
    "  # HINT: use tf.reduce_mean and tf.abs\n",
    "\n",
    "  #l1_loss =  ...TODO...\n",
    "\n",
    "  # FILL THE CODE: compute generator loss as the sum of GAN loss and weighted\n",
    "  # L1 loss\n",
    "\n",
    "  #total_gen_loss =  ...TODO...\n",
    "\n",
    "  return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttGX7STO35p8"
   },
   "source": [
    "![Generator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gen.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWVQXVNMEbTg"
   },
   "source": [
    "# Build the Discriminator\n",
    "Discriminator is a PatchGAN\n",
    "  * Each block is (Conv -> BatchNorm -> LeakyRelu)\n",
    "  * shape of output after last layer is (bs,30,30,1)\n",
    "    * N.B.-> each 30x30 patch of the output classifies a 70x70 portion of the input (PatchGAN)\n",
    "  * Discriminator receives two inputs.\n",
    "    * Input_1 image and target image, which it should classify as **real**.\n",
    "    * Input_2 image and generated image, which it should classify as **fake**.\n",
    "    * We concatenate Input_1 and Input_2 together in the code (```# tf.concat([inp, tar], axis=-1)```)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADNN7OPNEdNk"
   },
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
    "  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
    "\n",
    "  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
    "\n",
    "  down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
    "  down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
    "  down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
    "\n",
    "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
    "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
    "\n",
    "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
    "\n",
    "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12L6FMgtFkxZ"
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TND0mE1HFmWs"
   },
   "outputs": [],
   "source": [
    "disc_out = discriminator([inp[tf.newaxis,...], gen_output], training=False)\n",
    "plt.imshow(disc_out[0,...,-1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLu5lxTlFx4J"
   },
   "source": [
    "## Discriminator Loss: FILL THE CODE\n",
    "The discriminator loss function takes 2 inputs:\n",
    "* real images\n",
    "  * real_loss is a sigmoid cross entropy loss of the real images and an array of ones(since these are the real images)\n",
    "* generated images\n",
    "  * generated_loss is a sigmoid cross entropy loss of the generated images and an array of zeros(since these are the fake images)\n",
    "* Total_loss is a sum of real_loss and generated_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8q1QHLMrG96E"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "  # FILL THE CODE:\n",
    "  # define the GAN discriminator loss remember ideally discriminator should classify\n",
    "  # as 0 fake images and as 1 real images\n",
    "  # HINT: use loss_object and tf.ones_like and tf.zeros_like\n",
    "\n",
    "  # loss related to real images\n",
    "  #real_loss = # ...TODO...\n",
    "\n",
    "\n",
    "  # loss related to fake images\n",
    "  #generated_loss = # ...TODO...\n",
    "\n",
    "  # FILL THE CODE:\n",
    "  # compute output loss as the sum of real and generated loss\n",
    "\n",
    "  #total_disc_loss = # ...TODO...\n",
    "\n",
    "  return total_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nq78AkQtHJyg"
   },
   "source": [
    "# Define the Optimizers and Checkpoint-saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tu3iHB0nHAff"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DdvnIlDHC9y"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuCEvKrMHMlv"
   },
   "source": [
    "# Generate Images\n",
    "\n",
    "Write a function to plot some images during training:\n",
    "  * We pass images from the test dataset to the generator\n",
    "  * The generator will then translate the input image into the output\n",
    "  * Last step is to plot the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGqVax0YHD_6"
   },
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, tar):\n",
    "  prediction = model(test_input, training=True)\n",
    "  plt.figure(figsize=(15,15))\n",
    "\n",
    "  display_list = [test_input[0], tar[0], prediction[0]]\n",
    "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "  for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.title(title[i])\n",
    "    # getting the pixel values between [0, 1] to plot it.\n",
    "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnbWBXPXHhBg"
   },
   "outputs": [],
   "source": [
    "for example_input, example_target in test_dataset.take(1):\n",
    "  generate_images(generator, example_input, example_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbWbpuj3H5mn"
   },
   "source": [
    "# Training: FILL THE CODE\n",
    "* For each example input generate an output\n",
    "* The discriminator receives the input_image and the generated image as the first input. The second input is the input_image and the target_image.\n",
    "* Calculate Generator_loss and discriminator_loss\n",
    "* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n",
    "* Then log the losses to TensorBoard.\n",
    "\n",
    "* pay attention for training = True option for generator and discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bx4f1-V7IKet"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dX4AYj8uILYw"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir=\"logs/\"\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XQMnvyXIOt0"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_image, target, epoch):\n",
    "\n",
    "  # Use tf.gradient tape to keep track of the generator and discriminator gradients\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "    # FILL THE code: compute the output of the generator (i.e. fake images)   \n",
    "    #gen_output = # ...TODO...\n",
    "\n",
    "    # FILL THE CODE: compute the output of the discriminator w.r.t. real images \n",
    "    #disc_real_output = # ...TODO...\n",
    "\n",
    "    # FILL THE CODE: compute the output of the discriminator w.r.t. fake images \n",
    "    #disc_generated_output = # ...TODO...\n",
    "\n",
    "    # compute the 2 losses\n",
    "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "  # FILL THE CODE: \n",
    "  # Compute the gradient of the generator loss w.r.t. the generator parameters \n",
    "\n",
    "  # HINT: gen_tape.gradient(...), disc_tape.gradient(...)  \n",
    "\n",
    "  #generator_gradients = # ...TODO...\n",
    "\n",
    "    # Compute the gradient of the discriminator loss w.r.t. the discriminator parameters\n",
    "\n",
    "  #discriminator_gradients # ...TODO...\n",
    "\n",
    "  # Apply the gradient to the optimizer so that it can update the model accordingly  \n",
    "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          generator.trainable_variables))\n",
    "  \n",
    "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              discriminator.trainable_variables))\n",
    "\n",
    "  with summary_writer.as_default():\n",
    "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
    "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
    "    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
    "    tf.summary.scalar('disc_loss', disc_loss, step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKwszdZ7IQYc"
   },
   "source": [
    "Actual training loop:\n",
    "\n",
    "* Iterates over the number of epochs.\n",
    "\n",
    "* On each epoch it clears the display, and runs generate_images to show it's progress.\n",
    "* On each epoch it iterates over the training dataset, printing a '.' for each example.\n",
    "* It saves a checkpoint every 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eeMqVkOtIZLT"
   },
   "outputs": [],
   "source": [
    "def fit(train_ds, epochs, test_ds):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    for example_input, example_target in test_ds.take(1):\n",
    "      generate_images(generator, example_input, example_target)\n",
    "    print(\"Epoch: \", epoch)\n",
    "\n",
    "    # Train\n",
    "    for n, (input_image, target) in train_ds.enumerate():\n",
    "      print('.', end='')\n",
    "      if (n+1) % 100 == 0:\n",
    "        print()\n",
    "      train_step(input_image, target, epoch)\n",
    "    print()\n",
    "\n",
    "    # saving (checkpoint) the model every 20 epochs\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
    "                                                        time.time()-start))\n",
    "  checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjiT_irZIeHI"
   },
   "source": [
    "This training loop saves logs you can easily view in TensorBoard to monitor the training progress. Working locally you would launch a separate tensorboard process. In a notebook, if you want to monitor with TensorBoard it's easiest to launch the viewer before starting the training.\n",
    "\n",
    "To launch the viewer paste the following into a code-cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovnahmdvIeqy"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XXHQxv3I9VZ"
   },
   "source": [
    "Run the training loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcyZldPzI-5w"
   },
   "outputs": [],
   "source": [
    "fit(train_dataset, EPOCHS, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5Oqx9vsJzwn"
   },
   "outputs": [],
   "source": [
    "display.IFrame(\n",
    "    src=\"https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw\",\n",
    "    width=\"100%\",\n",
    "    height=\"1000px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Z1dtd8pKqIJ"
   },
   "source": [
    "Interpreting the logs from a GAN is more subtle than a simple classification or regression model. Things to look for::\n",
    "\n",
    "* Check that neither model has \"won\". If either the gen_gan_loss or the disc_loss gets very low it's an indicator that this model is dominating the other, and you are not successfully training the combined model.\n",
    "\n",
    "* The value log(2) = 0.69 is a good reference point for these losses, as it indicates a perplexity of 2: That the discriminator is on average equally uncertain about the two options.\n",
    "\n",
    "* For the disc_loss a value below 0.69 means the discriminator is doing better than random, on the combined set of real+generated images.\n",
    "\n",
    "*For the gen_gan_loss a value below 0.69 means the generator i doing better than random at foolding the descriminator.\n",
    "\n",
    "*As training progresses the gen_l1_loss should go down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVnb4z4XK733"
   },
   "source": [
    "# Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfXhMNBeLAG2"
   },
   "outputs": [],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xe0WpJmuLThI"
   },
   "outputs": [],
   "source": [
    "# Run the trained model on a few examples from the test dataset\n",
    "for inp, tar in test_dataset.take(5):\n",
    "  generate_images(generator, inp, tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICz6fehZw_Jd"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "k4Mu1gZIBFN2",
    "EJ_CKHc0By-u",
    "rGInXDbsB3Yu",
    "Cw3MrUqq522f",
    "3iBl8eQrDh45",
    "WWVQXVNMEbTg",
    "RLu5lxTlFx4J",
    "nq78AkQtHJyg",
    "RuCEvKrMHMlv",
    "kbWbpuj3H5mn",
    "zVnb4z4XK733"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
